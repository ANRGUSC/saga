{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets==8.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saga.experiment.benchmarking.parametric import ParametricExperiment\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the experiment\n",
    "experiment = ParametricExperiment(trim=100) # trim to 100 instances per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the datasets for the experiment\n",
    "experiment.prepare(download_url=\"https://zenodo.org/records/10967223/files/dataset.zip?download=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "\n",
    "total_evals = 144000  # Total evaluations\n",
    "completed_evals = 0   # Initial completed evaluations\n",
    "start_time = datetime.now()  # Start time of the experiment\n",
    "last_update_percent = -100 # Last update percentage\n",
    "\n",
    "def print_progress(*args, **kwargs):\n",
    "    global completed_evals, total_evals, start_time, last_update_percent\n",
    "    completed_evals += 1  # Increment the count of completed evaluations\n",
    "    percent_complete = completed_evals / total_evals  # Calculate percentage completion\n",
    "\n",
    "    if last_update_percent + 0.001 <= percent_complete:  # Update progress if more than 1% progress has been made\n",
    "        now = datetime.now()  # Current time\n",
    "        elapsed_time = (now - start_time).total_seconds()  # Elapsed time in seconds\n",
    "        if percent_complete > 0:\n",
    "            estimated_completion_time = start_time + timedelta(seconds=elapsed_time / percent_complete)  # Estimated completion time\n",
    "        else:\n",
    "            estimated_completion_time = now + timedelta(hours=1)  # Default to 1 hour more if percent_complete is 0\n",
    "\n",
    "        estimated_completion_time_str = estimated_completion_time.strftime(\"%Y-%m-%d %H:%M:%S\")  # Format completion time\n",
    "\n",
    "        # Generate progress report\n",
    "        prog_report = {\n",
    "            \"progress\": f\"{percent_complete * 100:.2f}%\",\n",
    "            \"elapsed time\": str(timedelta(seconds=elapsed_time)),\n",
    "            \"estimated remaining time\": str(timedelta(seconds=elapsed_time / percent_complete - elapsed_time)) if percent_complete > 0 else \"calculating...\",\n",
    "            \"current time\": now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"estimated completion time\": estimated_completion_time_str\n",
    "        }\n",
    "        prog_report_str = \"\\n\".join([f\"{k}: {v}\" for k, v in prog_report.items()])  # Create a string of the progress report\n",
    "\n",
    "        # Clear the console and print the progress report\n",
    "        clear_output(wait=True)\n",
    "        print(prog_report_str, flush=True)  # Print new progress report\n",
    "\n",
    "        last_update_percent = percent_complete  # Update the last update percentage\n",
    "\n",
    "# Assume `experiment.run(progress_callback=print_progress)` initiates the task and calls `print_progress` regularly.\n",
    "experiment.run(progress_callback=print_progress)  # Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze the results\n",
    "experiment.analyze(\n",
    "    filetype=\"png\",\n",
    "    showfliers=False,\n",
    "    do_pareto_plots=True,\n",
    "    do_main_effect_plots=False,\n",
    "    do_dataset_plots=False,\n",
    "    do_interaction_plots=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_scatter_path = experiment.outputdir / \"pareto_scatter.png\"\n",
    "img = plt.imread(pareto_scatter_path)\n",
    "img_plot = plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "pareto_chart_path = experiment.outputdir / \"pareto_chart.png\"\n",
    "img = plt.imread(pareto_chart_path)\n",
    "img_plot = plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate All Plots\n",
    "Generate all other plots and see the results in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.analyze(\n",
    "    filetype=\"png\",\n",
    "    showfliers=False,\n",
    "    do_pareto_plots=True,\n",
    "    do_main_effect_plots=True,\n",
    "    do_dataset_plots=True,\n",
    "    do_interaction_plots=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
